{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the process of using machine learning algorithms to detect hate speech in text through the use of Natural Language Processing (NLP) techniques. \n",
    "\n",
    "Sources for the data include:\n",
    "\n",
    "- Hatebase.org API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction from Hatebase API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import requests\n",
    "from config import hatebase_api_key\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting variables to access the API and authenticate with key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_url = \"https://api.hatebase.org/4-4/authenticate\"\n",
    "auth_key = \"api_key={}\".format(hatebase_api_key)\n",
    "headers = {\n",
    "    'Content-Type': \"application/x-www-form-urlencoded\",\n",
    "    'cache-control': \"no-cache\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the API and retrieving use token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.request(\"POST\", auth_url, data=auth_key, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = response.json()[\"result\"][\"token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the API using the vocabulary endpoint. This query will extract the first page of English vocabulary deemed hatespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_url = \"https://api.hatebase.org/4-4/get_vocabulary\"\n",
    "lang = \"eng\"\n",
    "resp_format = \"json\"\n",
    "vocab_payload = \"token=\" + token + \"&format=\" + resp_format + \"&language=\" + lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_response = requests.request(\"POST\", vocab_url, data=vocab_payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_json = vocab_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_pages  = vocab_json[\"number_of_pages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the API using the sightings endpoint. This query will extract the first page of \"sightings\" of hateful terms. We will extract data from the year 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting_url = \"https://api.hatebase.org/4-4/get_sightings\"\n",
    "sighting_year = \"2019\"\n",
    "sighting_payload = \"token=\" + token + \"&year=\" + sighting_year + \"&format=\" + resp_format + \"&language=\" + lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting_response = requests.request(\"POST\", sighting_url, data=sighting_payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting_json = sighting_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sighting_pages2019 = sighting_json[\"number_of_pages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining function to query multiple pages of API based on the endpoint and store results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hatebase_vocab(url, vocab_pages):\n",
    "    answer = []\n",
    "    for page in range(1, vocab_pages):\n",
    "        print(page)\n",
    "        payload = \"token=\" + token + \"&format=\" + resp_format + \"&page=\" + page + \"&language=\" + lang\n",
    "        response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "        result = response.json()[\"result\"]\n",
    "        answer.append(result)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-570d10da68e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocab_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hatebase_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-c57a127832c2>\u001b[0m in \u001b[0;36mget_hatebase_vocab\u001b[1;34m(url, vocab_pages)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_hatebase_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab_pages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"token=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&format=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresp_format\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&page=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&language=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "vocab_list = get_hatebase_vocab(vocab_url, vocab_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_hatebase_info(endpoint, year=None, page):\n",
    "    #df = pd.DataFrame([])\n",
    "    #answer = None\n",
    "    #response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "    #for i, item in enumerate(response.json()['result']):\n",
    "        #print(i)\n",
    "        #df = df.append(pd.DataFrame(item, index=[i]))\n",
    "    #return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
